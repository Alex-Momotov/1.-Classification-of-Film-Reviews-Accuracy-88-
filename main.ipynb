{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Cleaning and TF-IDF Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from memory_profiler import memory_usage\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def memory_time(func):\n",
    "    \"\"\"Decorator which prints time and memory taken by the client function.\"\"\"\n",
    "    def inner(*args, **kwargs):\n",
    "        t_before = time()\n",
    "        mem_before = memory_usage()[0]\n",
    "        result = func(*args, **kwargs)\n",
    "        t_after = time()\n",
    "        mem_after = memory_usage()[0]\n",
    "        t_elapsed = round(t_after - t_before, 2)\n",
    "        m_consumed = round(mem_after - mem_before, 2)\n",
    "        print(f'\\\"{func.__name__}\\\" finished, took: {t_elapsed} sec, consumed: {m_consumed} Mb')\n",
    "        print(f'\\tcurrent memory: {memory_usage()[0]} Mb')\n",
    "        return result\n",
    "    return inner\n",
    "\n",
    "@memory_time\n",
    "def parse_data(dataset='positive', sample_size=10000):\n",
    "    if dataset == 'positive':\n",
    "        path = r'data/2. Pickled/training/positives/'\n",
    "    elif dataset == 'negative':\n",
    "        path = r'data/2. Pickled/training/negatives/'\n",
    "    elif dataset == 'testing':\n",
    "        path = r'data/2. Pickled/testing/'\n",
    "    bodies = []\n",
    "    titles = []\n",
    "    for num, file in enumerate(os.listdir(path), start=1):\n",
    "        with open(path + file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            bodies.append(data['body'])\n",
    "            titles.append(data['title'])\n",
    "        if num == sample_size: break\n",
    "    return bodies, titles\n",
    "\n",
    "@memory_time\n",
    "def data_cleaning(data):\n",
    "    \"\"\"Punctuation removal, tokenisation, stemming.\"\"\"\n",
    "    sno = SnowballStemmer('english')                     # creating english stemmer\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')                 # configuring re module to remove all punctuation\n",
    "    stop_words = set(stopwords.words('english'))         # make a set of stopwords\n",
    "    clean_data = []\n",
    "    for text in data:\n",
    "        step1 = pattern.sub('', text)                            # removes all punctuation\n",
    "        step2 = step1.lower().split()                            # makes all lowercase and splits\n",
    "        step3 = [wrd for wrd in step2 if wrd not in stop_words]  # remove stop words\n",
    "        step4 = [sno.stem(word) for word in step3]               # returns stem of each word\n",
    "        clean_data.append(step4)\n",
    "    return clean_data\n",
    "\n",
    "@memory_time\n",
    "def create_lexicon(data, ignore_low, ignore_high):\n",
    "    \"\"\"Returns dictionary-like lexicon, removes given number of most/least frequently occuring words.\"\"\"\n",
    "    data_flat = [word for sublist in data for word in sublist]\n",
    "    data_counted = Counter(data_flat)\n",
    "    to_delete = []\n",
    "    for key, val in data_counted.items():\n",
    "        if val < ignore_low: to_delete.append(key)\n",
    "    for key in sorted(data_counted, key=data_counted.get, reverse=True)[:ignore_high]:\n",
    "        to_delete.append(key)\n",
    "    for key in to_delete:\n",
    "        del data_counted[key]\n",
    "    lexicon = list(data_counted)\n",
    "    lexicon.sort()\n",
    "    lexicon_dict = {}\n",
    "    for idx, word in enumerate(lexicon):\n",
    "        lexicon_dict[word] = idx\n",
    "    return lexicon_dict\n",
    "\n",
    "@memory_time\n",
    "def create_BOW_matrix(data, lexicon):\n",
    "    \"\"\"Creates and saves BOW matrix on disc, returns NumPy memory map of it.\"\"\"\n",
    "    list_of_counters = [Counter(text) for text in data]\n",
    "    BOW_matrix = np.zeros([len(data),len(lexicon)], dtype = np.float16)\n",
    "    for index, counter_obj in enumerate(list_of_counters):\n",
    "        for key, val in counter_obj.items():\n",
    "            try:\n",
    "                word_idx = lexicon[key]\n",
    "                BOW_matrix[index,word_idx] = val\n",
    "            except KeyError: pass\n",
    "    np.save(\"Processing stages/BOW_matrix.npy\", BOW_matrix)\n",
    "    BOW_matrix = np.load(\"Processing stages/BOW_matrix.npy\", mmap_mode='r+')\n",
    "    return BOW_matrix\n",
    "\n",
    "@memory_time\n",
    "def create_TFIDF_matrix(BOW_matrix):\n",
    "    \"\"\"Converts BOW to TF-IDF matrix saved on disc, returns NumPy memory map of it.\"\"\"\n",
    "    def idf(column):\n",
    "        count = np.count_nonzero(column)\n",
    "        if count > 0:\n",
    "            idf_score = np.log(len(column) / count, dtype=np.float32)\n",
    "        else:\n",
    "            idf_score = 0\n",
    "        return idf_score\n",
    "    idf_scores = np.apply_along_axis(idf, 0, BOW_matrix)\n",
    "    TFIDF_matrix = idf_scores * BOW_matrix\n",
    "    np.save(\"Processing stages/TFIDF_matrix.npy\", TFIDF_matrix)\n",
    "    TFIDF_matrix = np.load(\"Processing stages/TFIDF_matrix.npy\", mmap_mode='r')\n",
    "    return TFIDF_matrix\n",
    "\n",
    "@memory_time\n",
    "def normalise_unit_vec(BOW_matrix):\n",
    "    \"\"\"Converts TF-IDF feature vectors into unit vectors, returns NumPy memory map of the new matrix.\"\"\"\n",
    "    sq = ((BOW_matrix ** 2).sum(1)) ** 0.5\n",
    "    TFIDF_matrix_normed = BOW_matrix / sq[:, None]\n",
    "    np.save(\"Processing stages/TFIDF_matrix_normed.npy\", TFIDF_matrix_normed)\n",
    "    TFIDF_matrix_normed = np.load(\"Processing stages/TFIDF_matrix_normed.npy\", mmap_mode='r')\n",
    "    return TFIDF_matrix_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"parse_data\" finished, took: 0.33 sec, consumed: 5.23 Mb\n",
      "\tcurrent memory: 99.671875 Mb\n",
      "\"parse_data\" finished, took: 0.33 sec, consumed: 4.17 Mb\n",
      "\tcurrent memory: 103.83984375 Mb\n"
     ]
    }
   ],
   "source": [
    "sample_size = 2900\n",
    "data_pos, titles_pos = parse_data(dataset='positive', sample_size=sample_size)\n",
    "data_neg, titles_neg = parse_data(dataset='negative', sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning - tokenisation, stemming, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"data_cleaning\" finished, took: 6.32 sec, consumed: 25.67 Mb\n",
      "\tcurrent memory: 129.6015625 Mb\n",
      "\"data_cleaning\" finished, took: 5.13 sec, consumed: 21.34 Mb\n",
      "\tcurrent memory: 150.9375 Mb\n"
     ]
    }
   ],
   "source": [
    "clean_data_pos = data_cleaning(data_pos)\n",
    "clean_data_neg = data_cleaning(data_neg)\n",
    "data_all = clean_data_pos + clean_data_neg\n",
    "titles_all = titles_pos + titles_neg\n",
    "\n",
    "positive_labels = [1 for x in range(len(titles_pos))]\n",
    "negative_labels = [0 for y in range(len(titles_neg))]\n",
    "target = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_lexicon\" finished, took: 0.27 sec, consumed: 2.24 Mb\n",
      "\tcurrent memory: 153.7734375 Mb\n",
      "lexicon length: 15463\n"
     ]
    }
   ],
   "source": [
    "lexicon = create_lexicon(data_all, 2, 20)\n",
    "print('lexicon length:', len(lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_BOW_matrix\" finished, took: 1.04 sec, consumed: -0.83 Mb\n",
      "\tcurrent memory: 152.953125 Mb\n",
      "BOW matrix shape: (5800, 15463)\n",
      "<class 'numpy.core.memmap.memmap'>\n"
     ]
    }
   ],
   "source": [
    "BOW_matrix = create_BOW_matrix(data_all, lexicon)\n",
    "print('BOW matrix shape:', BOW_matrix.shape)\n",
    "print(type(BOW_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert BOW to TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_TFIDF_matrix\" finished, took: 2.72 sec, consumed: 171.2 Mb\n",
      "\tcurrent memory: 324.1796875 Mb\n"
     ]
    }
   ],
   "source": [
    "TFIDF_matrix = create_TFIDF_matrix(BOW_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise feature-vectors within TF-IDF as unit vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"normalise_unit_vec\" finished, took: 1.94 sec, consumed: 342.16 Mb\n",
      "\tcurrent memory: 666.34375 Mb\n"
     ]
    }
   ],
   "source": [
    "TFIDF_matrix_normed = normalise_unit_vec(TFIDF_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_all\n",
    "del clean_data_pos\n",
    "del clean_data_neg\n",
    "del data_pos\n",
    "del data_neg\n",
    "del TFIDF_matrix\n",
    "del BOW_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Centroid score: 0.8720689655172414\n"
     ]
    }
   ],
   "source": [
    "clf_nc = NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
    "clf_nc.fit(TFIDF_matrix_normed, target)\n",
    "centroid_score = cross_val_score(clf_nc, TFIDF_matrix_normed, target, cv=5).mean()\n",
    "print(\"Nearest Centroid score:\", centroid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent score: 0.8877586206896553\n"
     ]
    }
   ],
   "source": [
    "clf_sgd = SGDClassifier(random_state=46, max_iter=45, tol=0.001)\n",
    "clf_sgd.fit(TFIDF_matrix_normed, target)\n",
    "sgd_score = cross_val_score(clf_sgd, TFIDF_matrix_normed, target, cv=5).mean()\n",
    "print(\"Stochastic Gradient Descent score:\", sgd_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Analysing fail cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load new data that classifier doesn't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"parse_data\" finished, took: 0.33 sec, consumed: 2.77 Mb\n",
      "\tcurrent memory: 147.58984375 Mb\n",
      "\"data_cleaning\" finished, took: 0.35 sec, consumed: 0.02 Mb\n",
      "\tcurrent memory: 144.77734375 Mb\n"
     ]
    }
   ],
   "source": [
    "del TFIDF_matrix_normed\n",
    "data_experiment, titles_experiment = (parse_data(dataset='positive', sample_size=3000))\n",
    "data_experiment = data_experiment[-100:]\n",
    "titles_experiment = np.array(titles_experiment[-100:])\n",
    "clean_data_experiment = data_cleaning(data_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make BOW matrix, convert to TF-IDF representation, transform to unit vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_BOW_matrix\" finished, took: 0.15 sec, consumed: 0.27 Mb\n",
      "\tcurrent memory: 145.0546875 Mb\n",
      "\"create_TFIDF_matrix\" finished, took: 0.26 sec, consumed: 3.0 Mb\n",
      "\tcurrent memory: 148.05859375 Mb\n",
      "\"normalise_unit_vec\" finished, took: 0.18 sec, consumed: 11.8 Mb\n",
      "\tcurrent memory: 159.86328125 Mb\n",
      "(100, 15463)\n"
     ]
    }
   ],
   "source": [
    "BOW_matrix_experiment = create_BOW_matrix(clean_data_experiment, lexicon)\n",
    "TFIDF_experiment = create_TFIDF_matrix(BOW_matrix_experiment)\n",
    "TFIDF_norm_experiment = normalise_unit_vec(TFIDF_experiment)\n",
    "print(TFIDF_norm_experiment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict new data items with SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores_experiment = clf_sgd.predict(TFIDF_norm_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show file numbers and movie titles of some cases where our classifier failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005 Logan\n",
      "3009 Allied\n",
      "3013 Catch Me If You Can\n",
      "3020 Charade\n",
      "3022 Zero Dark Thirty\n",
      "3029 Logan\n",
      "3040 Silence\n",
      "3046 The Wolf of Wall Street\n",
      "3049 Harry Potter and the Order of the Phoenix\n",
      "3064 Moonlight\n",
      "3071 Logan\n",
      "3082 The Hateful Eight\n",
      "3083 The Hunt\n",
      "3084 Mutiny on the Bounty\n",
      "3088 Get Out\n",
      "3092 Prometheus\n",
      "3093 Fantastic Beasts and Where to Find Them\n"
     ]
    }
   ],
   "source": [
    "mask_scores = predicted_scores_experiment == 0\n",
    "doc_numbers = [3000 + x for x, y in enumerate(mask_scores, start=1) if y]\n",
    "failed_case_titles = titles_experiment[mask_scores]\n",
    "for doc_num, title in zip(doc_numbers, failed_case_titles):\n",
    "    print(doc_num, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Using trained classifier to predict new movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete existing memory-map matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del BOW_matrix_experiment\n",
    "del TFIDF_experiment\n",
    "del TFIDF_norm_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"parse_data\" finished, took: 0.24 sec, consumed: 0.73 Mb\n",
      "\tcurrent memory: 145.796875 Mb\n",
      "\"data_cleaning\" finished, took: 3.73 sec, consumed: 1.84 Mb\n",
      "\tcurrent memory: 147.640625 Mb\n",
      "1719\n"
     ]
    }
   ],
   "source": [
    "data_new, titles_new = parse_data(dataset='testing')\n",
    "clean_data_new = data_cleaning(data_new)\n",
    "print(len(clean_data_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bag of words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_BOW_matrix\" finished, took: 0.35 sec, consumed: 1.2 Mb\n",
      "\tcurrent memory: 148.8671875 Mb\n",
      "(1719, 15463)\n"
     ]
    }
   ],
   "source": [
    "BOW_matrix_new = create_BOW_matrix(clean_data_new, lexicon)\n",
    "print(BOW_matrix_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert BOW to TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"create_TFIDF_matrix\" finished, took: 1.38 sec, consumed: 50.7 Mb\n",
      "\tcurrent memory: 199.56640625 Mb\n"
     ]
    }
   ],
   "source": [
    "TFIDF_new = create_TFIDF_matrix(BOW_matrix_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TF-IDF to unit vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"normalise_unit_vec\" finished, took: 1.15 sec, consumed: 202.82 Mb\n",
      "\tcurrent memory: 402.390625 Mb\n"
     ]
    }
   ],
   "source": [
    "BOW_matrix_idf_norm_new = normalise_unit_vec(TFIDF_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (data_new)\n",
    "del (clean_data_new)\n",
    "del (BOW_matrix_new)\n",
    "del (TFIDF_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores_nc = clf_nc.predict(BOW_matrix_idf_norm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores_sgd = clf_sgd.predict(BOW_matrix_idf_norm_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of good and bad movie predictions with each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 1057, 0: 662})\n",
      "Counter({1: 1111, 0: 608})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(predicted_scores_nc))\n",
    "print(Counter(predicted_scores_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percent matched predictions between two classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent matches 89.52879581151832\n"
     ]
    }
   ],
   "source": [
    "matches = 0.\n",
    "for x, y in zip(predicted_scores_nc, predicted_scores_sgd):\n",
    "    if x == y: matches += 1\n",
    "print('Percent matches', (matches / len(predicted_scores_sgd)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate new data into two lists of good and bad movies using SGD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719\n"
     ]
    }
   ],
   "source": [
    "titles_new = np.array(titles_new)\n",
    "mask_good_movies = predicted_scores_sgd == 1\n",
    "mask_bad_movies = predicted_scores_sgd == 0\n",
    "good_movies = titles_new[mask_good_movies]\n",
    "bad_movies = titles_new[mask_bad_movies]\n",
    "print(len(good_movies) + len(bad_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
